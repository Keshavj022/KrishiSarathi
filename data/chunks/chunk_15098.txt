 To the extent possible, questions about pulse production were iden- tical across surveys. The exact wording is provided in Appendix A. Enumerators in both surveys were instructed to speak to the primary farmer in the household, who had previously been identified in the midline survey. This individual was the respondent in 84% of in-person and 81% of phone surveys. We interpret differences in the difficulty of reaching the desired respondent to be an inherent feature of data collection, and therefore treat it as one channel through which survey mode effects may operate. While both surveys were administered in parallel, the same household was typically not contacted by both modes on the same day. On average, the in-person survey was conducted 7 days after the phone survey, but differences range from 13 days earlier to 26 days later. In Appendix A we verify responses are not systematically related to this variation in timing. The upper tail of all production responses are Winsorized to the 95% level independently by crop and by mode to match how data would have been treated had either survey been conducted in isolation. This study presents two types of comparisons between in-person and phone survey responses. First, we compare moments in the distribution of self-reported production volume across survey mode. We report the mean, variance, and value at each decile for the four most common pulse varieties, restricting to households that reported positive area planted at midline and were therefore eligible for both surveys. This comparison reveals how inferences about population outcomes differ by survey mode inclusive of any bias introduced by differential attrition by survey respondents. We next decompose differences in distribution into selection and mode effects. This analysis leverages the fact that 711 households were contacted for both in-person and phone surveying. Out of these, 584 responded to both modes of contact, and in 429 cases the exact same individual answered each time. Variation in self-reported production volume within this overlapping sample can be attributed purely to survey mode, and the characteristics of non-respondents provide ev- idence about differential attrition bias. We also explore respondent engagement using evidence of rounding to the nearest five or ten. Second, we investigate how survey mode affects program evalu- ation. Here we estimate the intention-to-treat (ITT) effect on pulse production separately within each survey, represented by 𝛽in 𝑌𝑖= 𝛽𝑇𝑖+ 𝑋𝑖𝛿+ 𝛾𝑏(𝑖) + 𝜖𝑖 (1) where 𝑌𝑖represents production for household 𝑖living in block (sub- district) 𝑏(𝑖), 𝑇𝑖is a dummy indicating treatment status, 𝑋𝑖is a vector of household controls, and 𝛾𝑏(𝑖) are block-level fixed effects. The coefficient of interest 𝛽corresponds to the effect of treatment, and standard errors are clustered at the village level.