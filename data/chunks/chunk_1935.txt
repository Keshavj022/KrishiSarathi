 This approach is prominently employed in sophisticated machine learning domains, including applications like Autonomous Machinery Control (Agriculture robot), Irrigation Optimization etc. Machine Learning Types Semi-Supervised Learning Supervised Learning Figure 19: Basic Block Diagram of Machine learning® Shttps: //www.newtechdojo .com/list-machine-learning—algorithms / TEC 31228:2024 Telecommunication Engineering Centre 29 Revolutionizing Agriculture: The Digital Transformation of Farming Technical Report TEC 31228:2024 Telecommunication Engineering Centre 30 Some commonly used models in predicting crop traits are listed below- Table 5: List of some commonly used models in predicting crop traits S. No. Algorithm Features 1. Kernel ridge regression (KRR) Kernel is introduced to RR. Uses squared error loss. Faster for medium-sized datasets. Matrix inversion 2. Least squares linear regression (LSLR) Model the relationship between a dependent variable and one or more independent variables. Does not consider the complexity of data. 3. Neural network (NN) Approach that uses a standard back-propagation algorithm applied to a set of input, hidden, and output layers. Predicts the results for unknown datasets. Requires labelled data for the training process. The training of the network takes time. 4. Support vector regression (SVR) Works on the concept of maximizing the margins. Generates a decision boundary with maximum separation. Proves helpful when multiple heterogeneous classes are available. 5. Extreme learning machine (ELM) Learning algorithm for single-layered feed-forward neural network. Fast learning. Computationally scalable. Independent from the tuning process. Evaluation speed is low. 6. Bagging trees (BaTs) General-purpose procedure for reducing the variance of a statistical learning method. Makes predictions on the tree’s out-of-bag observations. Multiple trees can be trained simultaneously. All the trees trained on different bootstrap samples are correlated. 7. Boosting trees (BoTs) Transforms weak decision trees (called weak learners) into strong learners. Tends to overfit. Better than random predictions. Good at handling tabular data with numerical features. Able to capture nonlinear interactions between the features and the target. Not designed to work with very sparse features. 8. Random Forest An ensemble approach uses decision trees. Creates multiple decision trees on different data samples and then predict the data from each subset. Finally the forest (group of random trees) is averaged. 9. Gaussian Process regression (GPR) Probabilistic (Bayesian) approach, An additional quantitative measurement of prediction accuracy in terms of uncertainty estimates, Use of kernels or covariance functions to reduce the processing time.