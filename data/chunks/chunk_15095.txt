 We show that heterogeneity in the method of contact may introduce bias into comparisons of survey outcomes across populations. Such bias can undermine conclusions about differences between study populations or about the evolution of outcomes within a population over time, such as in subsequent rounds of a panel or repeated cross-sectional survey. To make such comparisons viable, it is necessary to establish reliable indicators that link data across survey modes. We find this issue to be less of a concern for program evaluation. Our findings also highlight a tradeoff in the use of phone surveying for program evaluation. While it may be cheaper to conduct surveys by phone than in person, the resulting data may be noisier. In such cases, phone-based data collection necessitates larger samples to achieve the same power, offsetting some of the cost savings. In our context there is substantial heterogeneity in the breakeven point: depending on the crop, the phone sample would have needed to be 1.2–10.7 times larger than the in-person sample to estimate treatment effects with the same precision. In general, it would be prudent for researchers to consider noise specific to survey method when calculating power. Evidence on how survey mode affects data reliability most com- monly focuses on self-reported health indicators. Investigators report mixed results on the correspondence between in-person and phone responses, and those showing statistical differences draw no systematic conclusions about types of indicators subject to mode effects or direc- tion of bias (Greenfield et al., 2000; Biemer, 2001; Scherpenzeel and Eichenberger, 2001; St-Pierre and Béland, 2004; Nord and Hopwood, 2007; Ferreira et al., 2011; Mahfoud et al., 2015; Greenleaf et al., 2020). Other comparisons include phone-based measures of consumer valuation (Maguire, 2009; Szolnoki and Hoffmann, 2013), microenter- prise data (Garlick et al., 2020), and school performance (Crawfurd et al., 2021). In developing-country agriculture, Kilic et al. (2021) uncover a similar pattern to ours of greater self-reported production by phone than in person among tuber farmers in Malawi.1 Our analysis extends this literature in three ways. First, the overlap- ping sample of respondents allows for within-household estimation of survey mode effects. Only Mahfoud et al. (2015) include this feature, but prime for consistency by advertising phone contact as a check on prior in-person responses.2 Second, while most existing work tests for 1 A complementary application of mobile phone data avoids survey- ing altogether and draws inferences about household-level outcomes from metadata (see Blumenstock et al., 2015). 2 Biemer (2001), Nord and Hopwood (2007) analyze panel data from national statistical offices where the first survey round is conducted in person and subsequent rounds by phone, but this structure does not allow separate identification of survey mode and time effects within household.