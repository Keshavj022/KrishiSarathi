 bias in sample means, we also report differences in precision and at var- ious production quantiles. In particular, our finding of greater variance in phone-based data, consistent with a recent study of microenter- prises (Garlick et al., 2020), can inform sample size calculations in research design. Third, we investigate how the mode used for data collection affects program evaluation in agriculture. Crawfurd et al. (2021) reach a similar conclusion that survey mode affects measure- ment of student test scores on average, but does not bias evaluation of an educational intervention. Research interpreting phone survey data is especially timely fol- lowing COVID-19 disruptions that forced remote data collection. To accurately quantify the evolution of economic outcomes through the pandemic and beyond, researchers must find ways to relate outcomes across surveys (e.g., Egger et al., 2021; Josephson et al., 2021; Barker et al., 2023, for successful examples). To the extent that lessons learned from the large-scale use of remote data collection during the pan- demic (Gourlay et al., 2021; Zezza et al., 2022) enable these practices to remain in place in the future, it will be important to develop methods to establish comparability between pre- and post-pandemic surveys. Our investigation also relates to the growing body of work on how to aggregate evidence across studies. Many policy evaluations take place in idiosyncratic contexts, and organizations such as 3ie3 and Cochrane Reviews4 devote substantial resources to drawing general conclusions about policy impacts. Meager (2019) provides an empir- ical framework for evidence aggregation that disentangles average policy impacts, context-specific heterogeneity, and sampling variation; and Pritchett and Sandefur (2015) argue heterogeneity across contexts can threaten external validity moreso than poor identification. In this paper we demonstrate how and when the mode of survey can intro- duce study-specific heterogeneity in measured outcomes that is largely uninformative for policy decisions. 2. Data and methodology Data for this study come from two overlapping randomized evalua- tions of an agricultural extension program to promote pulse cultivation in Bihar, India. The program consisted of offering farmers subsidized inputs to accelerate adoption combined with high-intensity extension to teach best practices through learning-by-doing over a period of two years. In this paper we analyze data on pulse production collected in the first-year endline, the only round involving both phone and in-person data collection. The initial intervention began in May 2017, followed by a pre- harvest midline survey conducted in person in December 2017. The 2346 midline respondents, selected at random from the 6971 evalua- tion households, comprise the sampling frame for the current study. At midline, all sample households reported on demographic charac- teristics and pre-harvest farm area devoted to pulses.