 Moreover, the difference is so small that even if rounding caused respondents to double their self-reported production, it would only raise average production by 1% more by phone relative to in-person, well below the 14%–68% gaps reported in Fig. 1. These magnitudes imply that, while participants clearly round their responses, the influence of this behavior on differences by survey mode must be small. Appendix A presents further evidence that respondent engagement does not appear to decay at differential rates between survey modes for the outcomes studied in this paper. However, we add the caveat that the pulse module was the first module asked in both surveys after consent and respondent identification, so it is unclear how well this finding would generalize over longer durations. Journal of Development Economics 166 (2024) 103199 7 E. Anderson et al. Fig. 3. Right-most digit frequencies by survey mode. Notes: Fraction of non-zero responses with each value for rightmost digit by crop and by survey mode. 4. Treatment effect estimation Results so far indicate population comparisons between surveys may be undermined by systematic differences caused by survey mode. In this section, we investigate how survey mode affects impact evaluation. This analysis is informative for researchers selecting a method of data collection or comparing results generated using different methods, for example when making inferences about how treatment effects evolve over time within a population. For this analysis, we report impact evaluation results according to estimation of (1) separately by crop and survey mode. Estimation is straightforward for the in-person sample as it is drawn uniformly at random from the sampling frame. Production quantity is as reported for survey respondents with positive area planted and assumed to be zero for respondents with zero area planted. Regression following (1) produces a treatment effect estimate inclusive of attrition bias caused by survey non-response. Comparable estimation in the phone sample is confounded by the fact that enumerators did not attempt to contact households with zero area devoted to pulses at midline. Therefore, the sample con- sists of a subset of households – those with positive area planted – subject to the attrition pressures induced by phone surveying and a complementary subset – those with no area planted – with known production volume but an unknown phone response rate. These groups are endogenously determined because area planted at midline may be affected by treatment. To estimate the effect of treatment in the phone sample, we run a weighted least squares regression following (1). Households that re- sponded to the phone survey are assigned a weight of 1, and households with zero area planted are assigned a weight of 0.83 corresponding to the response frequency among surveyed households.