 This dataset possesses a unique quality as it encompasses diverse geographical conditions and a wide range of crops, B. Dey et al. Heliyon 10 (2024) e25112 3 thereby exhibiting the potential to be utilized in various regions across the world having similar environmental conditions. 2.2. Prediction of crop using ML techniques The study aimed to predict the selection of crops based on several factors, including NPK fertilizer, soil pH, and climatic factors, using regression algorithms (Fig. 1). To accomplish this objective, five machine learning algorithms were utilized including SVM, random forest (RF), eXtreme gradient boosting (XGBoost), K-nearest neighbors (KNN), and decision tree (DT). 2.2.1. Decision tree A decision tree is a method for categorizing crops based on attribute-value tests related to factors like NPK fertilizer, soil pH, and climate. These tests are used to create the tree, starting with a training set of data points, each with attribute values and a corre­ sponding crop category. The choice of which attribute to test is based on its ability to differentiate between crop categories, and this process is recursive, leading to various tree sizes. Pruning can be applied to prevent overfitting and improve the tree’s classification performance in terms of portability and generalization. GridSearchCV was used to fine-tuning and finding the optimal hyper­ parameters for the decision tree model. Specifically, max features = ’auto’, the best estimator, ccp alpha = 0.001, criterion = ’entropy’, random state = 2, and max depth = 5 were employed in the training of the decision tree model. 2.2.2. eXtreme gradient boosting XGBoost is widely regarded as an advancement in machine learning algorithms, owing to its integration of a gradient-boosted decision tree. This feature endows the model with superior flexibility, speed, and performance compared to other models. Decision tree ensembles are built using numerous decision tree models. It separates data based on features is akin to that of a tree model. With each iteration of the model fitting process, additional trees are incorporated to address and correct any prediction errors made by previous models. Each sample is allocated to a cluster of leaves in a tree that indicates a numerical weight based on the values of its input variables. In order to enhance the performance of the XGBoost model, we utilized several hyperparameters. The hyperparameters for the model were configured as follows: the learning rate was set to 0.1, max depth to 17, n estimators to 200, subsample rate was set to 0.5, gamma value was set to 0, and the seed was set to 50. The hyperparameters were tuned to ensure the best possible performance of the XGBoost model, taking into account factors such as speed, accuracy, and flexibility. 2.2.3. Support vector machine The SVM is a classification method that effectively reduces model complexity while accurately fitting the training data.