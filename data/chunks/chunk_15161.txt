 The approach operates by identifying linear hyperplanes that maximize the distance between two sides or edges of several hyperplanes, thereby minimizing the likelihood of generalization errors [41]. Reducing the number of support vectors (data point closest to the hyperplane), helps to simplify the model and reduce overfitting. By doing so, the hyperplane becomes less dependent on these support vectors, which increases the potential for generalization. To build an ideal separation plane from the starting data in a high-dimensional feature space, SVM employs an appropriate kernel function. In this study, we used a complexity constant of C = 5, to determine the tolerance for misclassification. A high value of C may lead to overfitting issues, while a moderate value may result in overgeneralization. Furthermore, in this study, we utilized a gamma value of 0.1 and a kernel of ’rbf’. The ’rbf’ kernel function denotes the radial basis function, a widely employed technique in SVM to transform the data into a higher-dimensional space. The utilization of SVM enables the efficient identification of non-linear correla­ tions among the features, thereby augmenting its precision and efficacy. 2.2.4. Random forest The random forest (RF) offers a distinct advantage over other ML models by effectively reducing generalization error. This Fig. 1. Outline the comprehensive workflow for conducting analysis and recommending cropping strategies through the utilization of machine learning techniques. B. Dey et al. Heliyon 10 (2024) e25112 4 Classifier, an ensemble learning method, is utilized to classify data by generating a forest consisting of a varying number of trees and subsequently averaging the predictions of each individual decision trees. Unlike basic DT algorithms, which are rule-based and rely solely on rules for predicting data sets, RF classifiers employ a different approach by randomly partitioning the functions instead of utilizing the Gini index or gaining weight to estimate the root-node. Each individual tree generates a prediction, and the ultimate outcome is determined by the class with the highest number of votes. A criterion refers to a mathematical function that is utilized to evaluate the quality of a split. The entropy criterion is unique to trees, whereas the Gini criterion supports the Gini impurity. In this study, the RF model was tuned with the following parameters: a maximum depth of 6, maximum features of 5, minimum samples split of 4, random state of 0, and n estimator of 15. 2.2.5. K-nearest neighbors The K-nearest neighbors’ approach is a commonly used method for identifying the k number of training samples in a given training set that exhibits the highest degree of similarity to a target object [42]. Once identified, the approach assigns the dominant category to the target object by leveraging the category of the K training samples.