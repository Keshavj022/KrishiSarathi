 Adequate trees to normalise the error, but using too many trees is wasteful, especially when working with large data sets. The ‘mytr’ is the number of variables to sample at random as candidates in each split. ‘sampsize’, is the set of samples used to train. ‘nodsize’ is the terminal nodes must have a minimum number of samples available. ‘maxnodes’ is the terminal nodes must have a maximum number of samples avail­ able. In this study, ‘ntree’ i.e the number of trees is 200, ‘mytr’ i.e the Table 3 Relative importance scale for FCOPRAS. Importance Linguistic form Rating Extremely low EL 0 0 0.1 Very low VL 0 0.1 0.3 Low L 0.1 0.3 0.5 Medium M 0.3 0.5 0.7 High H 0.5 0.7 0.9 Very High VH 0.7 0.9 1 Extremely high EH 0.9 1 1 Table 4 Assignment of rating to the Variable by Decision Makers. Variable Decision Maker I Decision Maker II Decision Maker III N 0.9 1.0 1.0 0.7 0.9 1.0 0.5 0.7 0.9 PO3− 4 0.9 1.0 1.0 0.9 1.0 1.0 0.5 0.7 0.9 K 0.9 1.0 1.0 0.7 0.9 1.0 0.5 0.7 0.9 Zn 0.5 0.7 0.9 0.3 0.5 0.7 0.7 0.9 1.0 S 0.7 0.9 1.0 0.7 0.9 1.0 0.9 1.0 1.0 Sd 0.5 0.7 0.9 0.3 0.5 0.7 0.7 0.9 1.0 Cu 0.3 0.5 0.7 0.5 0.7 0.9 0.9 1.0 1.0 B 0.5 0.7 0.9 0.3 0.5 0.7 0.5 0.7 0.9 Mn 0.3 0.5 0.7 0.5 0.7 0.9 0.7 0.9 1.0 EC 0.3 0.5 0.7 0.0 1.0 0.3 0.5 0.7 0.9 CEC 0.5 0.7 0.9 0.1 0.3 0.5 0.5 0.7 0.9 PH 0.9 1.0 1.0 0.7 0.9 1.0 0.9 1.0 1.0 BD 0.3 0.5 0.7 0.1 0.3 0.5 0.1 0.3 0.5 MI 0.1 0.3 0.5 0.3 0.5 0.7 0.5 0.7 0.9 STEX 0.5 0.7 0.9 0.3 0.5 0.7 0.9 1.0 1.0 S. Saha and P. Mondal Artificial Intelligence in Geosciences 3 (2022) 179–191 184 number of variables to sample at random as candidates in each split is 1 (Liaw and Wiener, 2002), minsampsize i.e the set of samples used to train (min) is 3, max number of levels in each decision tree 80, ‘maxnodes’ i.e the terminal nodes (max) and ‘nodsize’ i.e the terminal nodes (min) is remain default. 3.2.2.2. Multilayer perceptron algorithms. One of the most basic feed- forward neural networks is the Multilayer Perceptron. Multilayer per­ ceptrons are bidirectional neural networks in which the inputs are propagated forward and the weights are propagated backward (Brei­ man, 2001). A multilayer perceptron has an input layer and an output layer with one or more hidden layers. All MLPs connect all neurons in one layer to all neurons in the next layer. The input layer receives the input signals, and the output layer performs the intended task. All of the calculations are done by the hidden layers (Wang et al., 2021). 3.2.3. Effectiveness estimation of applied models/algorithms 3.2.3.1. 3.2.3.1. ROC (receiver operating characteristic curve). The Receiver Operating Characteristic Curve (ROC curve) is a graph that shows the output of a classification model at all classification levels. If the area under the ROC curve (AUC) values between 0.9 and 1 were considered excellent, the values between 0.8 and 0.9 were considered good, the values between 0.7 and 0.8 were considered fair, the values between 0.6 and 0.