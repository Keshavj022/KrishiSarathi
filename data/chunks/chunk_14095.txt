 f(x)=〈ω*ϕ(x) + b〉 (12) min(ω , b, ξ −, ξ+) = 1 2 *‖ω2‖ + c ∑ n i=1 (ξi−+ ξi+) (13) where ϕ(x) signifies the kernel function (k), three types of (k) poly­ nomial, radial basis, or linear function; b indicates the represents weights and basis support vectors; C suggests the penalty value specified earlier for the training error Eq. (13); ξi − and ξi + represent the lower and upper output limits of Eq.14 (Nasir et al., 2022). SVM has many merits, including ability to handle high-dimensional data, its effective­ ness in handling noisy data, and its ability to work efficiently with small amounts of data (Vapnik, 2013; Sarafaraz et al., 2024). This model is widely used in hydrological engineering and groundwater (Mohseni et al., 2024). SVM can process high-dimensional data when the features exceed the training examples and process inseparable classes when only the support vector affects the almost influence of outliers in the hyper­ plane (Nasir et al., 2022). The separation function used in support vector classification is a linear combination of kernels add to the support vector (Hassan et al., 2021). Nasir et al. (2022) used SVM model in a water quality classification study of 1679 sample data collected over the period of 2005 to 2014 from various States of India. 2.6.2. K- Nearest Neighbour model (KNN) K-Nearest Neighbour (KNN) was initially developed by the Evelyn Fix and Joseph Hodges in 1951 and then advanced by Thomas Cover Altman in 1992 (Hajihosseinlou et al., 2024). KNN is one of the nonparametric classification methods. This is known for its broad and simple algorithms. It can store any issue or case it can access and group them into new clusters according to their similarity. KNN works on the concept of identifying adjacent neighbours and classifying them based on their similarity. KNN uses the Euclidean distance technique to find the best similar data in a group (Zamri et al., 2022). Meanwhile using the KNN technique nearest fixed neighbours are M.R. Hari Raj et al. Computers and Electronics in Agriculture 229 (2025) 109932 6 permitted to vote during the classification of unknown data denoted by integer “k”, where “k” is a positive number. The training data class closest to the unknown data when k = 1 is classified. Although it is a simple ML technique, the nearest KNN algorithm works well for solving traditional classification problems (Ramadhani et al., 2021; Gazalba and Reza2017; Rezaei et al., 2011). 2.6.3. Model building and normalization of data The development of ML models for irrigation water quality valuation is covered in this section. The SVM and KNN models were created using Google Collab open-source online compiler software. The performance of models can be misled by different ranges of variables in the data, which give more weight to the large-range variable than to the short- range variable when predicting the target variable.